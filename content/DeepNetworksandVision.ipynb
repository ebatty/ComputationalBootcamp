{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyODubYoDn7DtxTnoREqnUq8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebatty/ComputationalBootcamp/blob/master/content/DeepNetworksandVision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Neural Networks and Vision"
      ],
      "metadata": {
        "id": "Qr64e5Zck0e8"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFZn7JDCtKkB"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.io as sio\n",
        "import ipywidgets as widgets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XvWltebsBWK"
      },
      "source": [
        "Execute the next cell to download and process the data. It takes a few minutes so do this first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjGFKJBU_Y7a"
      },
      "source": [
        "import requests\n",
        "import io\n",
        "\n",
        "response = requests.get('https://osf.io/wh3nb/download')\n",
        "IT_resps = np.load(io.BytesIO(response.content)).reshape((7*7, 40, 168))  \n",
        "\n",
        "response = requests.get('https://osf.io/wcdsa/download')\n",
        "V4_resps = np.load(io.BytesIO(response.content)).reshape((7*7, 40, 128))  \n",
        "\n",
        "response = requests.get('https://osf.io/t2jqc/download')\n",
        "images = np.load(io.BytesIO(response.content)).astype('float64').reshape((7*7, 40, 256, 256, 3))\n",
        "\n",
        "images = np.moveaxis(images, 4, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kULtLoUE4DU7"
      },
      "source": [
        "We will be looking at neural data from https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003963\n",
        "\n",
        "The authors presented images to monkeys while recording from areas V4 and IT.\n",
        "\n",
        "**Images:** They presented images from 7 different categories (Animals, Cars, Chairs, Faces, Fruits, Planes, and Tables). In each category, they had 7 separate objects. They created 40 configurations of each object, showing it on top of different backgrounds and in different sizes, shapes, and rotations. They presented each of these 1960 images (7 categories x 7 objects x 40 configurations) around 40 times. \n",
        "\n",
        "You can see 5 examples for 4 of the 7 objects in the Animals category by executing the next cell. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3hArgUt4JEM",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute to visualize images\n",
        "fig, axes = plt.subplots(4, 5, figsize = (10, 8))\n",
        "\n",
        "for i_row in range(4):\n",
        "  for i_col in range(5):\n",
        "    axes[i_row][i_col].imshow(images[i_row, i_col, 0, :, :], cmap = 'gray')\n",
        "    axes[i_row][i_col].set(xticks = [], yticks = [])\n",
        "  axes[i_row][0].set(ylabel = f'Object {i_row}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8vTyt-G4JQ-"
      },
      "source": [
        "**Neural activity**: While presenting the above images, the authors recorded 168 cells from area IT and 128 cells from area V4. For each image, they summed the spiking responses of each neuron from 70 to 170 ms after the image presentation to get a response per neuron per image.  Note this removes any temporal information. They also normalized these values so don't be alarmed if you see negative values for neural responses.\n",
        "\n",
        "Execute the following cell to visualize an example neuron's responses to the 1960 images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtDeqvPC5hn2",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute to see example neural responses\n",
        "\n",
        "categories = np.array(['Animals', 'Cars', 'Chairs', 'Faces', 'Fruits', 'Planes', 'Tables'])\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "\n",
        "ax.plot(IT_resps[:, :, 1].reshape((-1,)), 'k')\n",
        "ylim = ax.get_ylim()\n",
        "for i in range(0, 1900, 7*40):\n",
        "  ax.plot([i, i], ylim, 'r')\n",
        "\n",
        "ax.set(xticks = np.arange(200, 1960, 7*40),\n",
        "       xticklabels = categories,\n",
        "       ylabel = 'Neural Response');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCO75mtbVrQU"
      },
      "source": [
        "## Section 1: Neural responses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3_4HTfxsRlm"
      },
      "source": [
        "### Exercise 1: RDM for area IT \n",
        "\n",
        "Let's look at how the different objects and categories are represented in area IT by constructing a representational dissimilarity matrix (RDM). The data is already ordered by category: the first seven rows are the animal images, the next seven are the car images, and so on.\n",
        "\n",
        "`IT_resps` is a 49 x 40 x 168 array. The first dimension is the object (7 categories x 7 objects per category = 49). The data is already ordered by category: the first seven rows are the animal images, the next seven are the car images, and so on. The second dimension is the 40 different configurations per category. And the third dimension corresponds to the 168 area IT neurons. \n",
        "\n",
        "**Step 1)** First, average the neural responses over the 40 different configurations of each object. What size array should you have? \n",
        "\n",
        "**Step 2)** Compute the representational dissimilarity matrix for this data and store it in a variable called `rdm`. This should be a 49 x 49 matrix where each entry equals 1 minus the correlation of the neural responses to two images (the image represented by the row number and the image represented by the column number). So the entry at row 4, column 8 is 1 minus the correlation of the responses to images 4 and 8.\n",
        "\n",
        "*Hint 1:* No need to vectorize this - looping over rows and columns is useful\n",
        "\n",
        "*Hint 2:* To compute the correlation coefficient between two numpy arrays, use np.corrcoef as follows:\n",
        "\n",
        "```\n",
        "example_x = np.random.randn(200,)\n",
        "example_y = np.random.randn(200,)\n",
        "np.corrcoef(example_x, example_y)[0, 1]\n",
        "```\n",
        "\n",
        "**Step 3)** Visualize the RDM. What sort of plot should you use? Try to label the different categories on the x axis and y axis. The ordering of categoies is in the variable `categories`\n",
        "\n",
        "*Hint 1:* You will need to set the ticks first (`ax.set(xticks=...)`) and then change what the labels of thos ticks should be (`ax.set_xticklabels(...)`)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPBEwkq5d7Dw"
      },
      "source": [
        "# Compute RDM for IT responses\n",
        "\n",
        "# Step 1) Average over 40 configs per object\n",
        "\n",
        "# Step 2) Compute RDM\n",
        "\n",
        "# Step 3) Visualize your RDM"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jzgjbm6EukhX",
        "cellView": "form"
      },
      "source": [
        "# @markdown Click here for the solution\n",
        "\n",
        "# Step 1) Average over 40 configs per object\n",
        "averaged_IT = np.mean(IT_resps, axis=1)\n",
        "\n",
        "# Step 2) Compute RDM\n",
        "rdm = np.zeros((49, 49))\n",
        "\n",
        "for i_row in range(rdm.shape[0]):\n",
        "  for i_col in range(rdm.shape[1]):\n",
        "    rdm[i_row, i_col] = 1 - np.corrcoef(averaged_IT[i_row, :], averaged_IT[i_col, :])[0, 1]\n",
        "\n",
        "# Step 3) Visualize rdm\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "im = ax.imshow(rdm, cmap = 'jet')\n",
        "for i_cat in range(0, 49, 7):\n",
        "  ax.plot([i_cat, i_cat], [0, 49], 'k')\n",
        "  ax.plot([0, 49], [i_cat, i_cat], 'k')\n",
        "ax.set(xticks = np.arange(4, 49, 7))\n",
        "ax.set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax.set(yticks = np.arange(4, 49, 7))\n",
        "ax.set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax.set(xlim = [0, 49], ylim = [49, 0])\n",
        "\n",
        "plt.colorbar(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sCq_LuTsTij"
      },
      "source": [
        "### Exercise 2: Interpreting RDMs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL1ORet_g2QX"
      },
      "source": [
        "In the cell below, you can see the RDMs for the V4 neural responses and the IT neural responses.\n",
        "\n",
        "Discuss the following questions:\n",
        "\n",
        "A) Within a category, does the population of IT neurons often respond similarly for different objects? What category is this especially true of? What does this tell you about the type of coding in area IT, e.g. the complexity of the features encoded?\n",
        "\n",
        "B) How does the V4 RDM differ from the IT RDM? What does this tell you about V4 processing as compared to IT processing?)**\n",
        "\n",
        "C) If you recorded from V1 neurons and constructed an RDM, what would you expect it to look like?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWC0Txsje3Sp",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute to visualize RDMs for area IT and V4\n",
        "from scipy.stats import zscore\n",
        "\n",
        "def compute_RDM(resp):\n",
        "  \"\"\"Compute the representational dissimilarity matrix (RDM)\n",
        "  Args:\n",
        "    resp (ndarray): S x N matrix with population responses to\n",
        "      each stimulus in each row\n",
        "  Returns:\n",
        "    ndarray: S x S representational dissimilarity matrix\n",
        "  \"\"\"\n",
        "\n",
        "  # z-score responses to each stimulus\n",
        "  zresp = zscore(resp, axis=1)\n",
        "\n",
        "  # Compute RDM\n",
        "  RDM = 1 - (zresp @ zresp.T) / zresp.shape[1]\n",
        "\n",
        "  return RDM\n",
        "\n",
        "V4_rdm = compute_RDM(np.mean(V4_resps, axis = 1))\n",
        "IT_rdm = compute_RDM(np.mean(IT_resps, axis = 1))\n",
        "\n",
        "fig, (ax, ax2, cax) = plt.subplots(ncols=3,figsize=(7,3), \n",
        "                  gridspec_kw={\"width_ratios\":[1,1, 0.05]})\n",
        "fig.subplots_adjust(wspace=0.3)\n",
        "im = ax.imshow(V4_rdm, vmin = 0, vmax = 1.75, cmap = 'jet')\n",
        "for i_cat in range(0, 49, 7):\n",
        "  ax.plot([i_cat, i_cat], [0, 49], 'k')\n",
        "  ax.plot([0, 49], [i_cat, i_cat], 'k')\n",
        "ax.set(xticks = np.arange(4, 49, 7))\n",
        "ax.set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax.set(yticks = np.arange(4, 49, 7))\n",
        "ax.set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax.set(xlim = [0, 49], ylim = [49, 0])\n",
        "ax.set(title = 'V4 RDM')\n",
        "\n",
        "im = ax2.imshow(IT_rdm, vmin = 0, vmax = 1.75, cmap = 'jet')\n",
        "for i_cat in range(0, 49, 7):\n",
        "  ax2.plot([i_cat, i_cat], [0, 49], 'k')\n",
        "  ax2.plot([0, 49], [i_cat, i_cat], 'k')\n",
        "ax2.set(xticks = np.arange(4, 49, 7))\n",
        "ax2.set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax2.set(yticks = np.arange(4, 49, 7))\n",
        "ax2.set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "ax2.set(xlim = [0, 49], ylim = [49, 0])\n",
        "ax2.set(title = 'IT RDM')\n",
        "fig.colorbar(im, cax=cax)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Click here for the solution\n",
        "\"\"\"\n",
        "A) Yes, the diagonal blocks that represent the same category en dark blue for area IT, \n",
        "meaning the responses are similar for different objects in the same category. \n",
        "This is especially true of faces. This shows that the IT neurons are not responding \n",
        "to low-level features such as edges or orientations, because those would differ a \n",
        "lot for different objects (and configurations) even in the same category. Instead, \n",
        "area IT is representing more high-level information about object identity.\n",
        "\n",
        "B) The V4 RDM has much less block-like structure. Let's take the diagonal blocks - \n",
        "those are not as fully blue as in the IT RDM. This means that the neural responses \n",
        "to objects in the same category are not as uniformly similar. There is much less \n",
        "category-level grouping in the responses. This shows that V4 is responding to \n",
        "lower-level features and not high-level category representation.\n",
        "\n",
        "C) I'd expect a V1 RDM to be even less structured than a V4 neurons - with very \n",
        "little discernable structure. V1 neurons respond to edges and these would be \n",
        "different even for the configurations of an object that we've already averaged \n",
        "over, let alone different objects in the same category.\n",
        "\"\"\";\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ngH0lWoSotQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKkYdZutVvLb"
      },
      "source": [
        "## Section 2: Convolutional neural network\n",
        "\n",
        "We will examine a convolutional neural network, specifically AlexNet, a network that was state-of-the-art at object recognition in 2012. Execute the following cell to load in our model, AlexNet. It consists of 5 convolutional layers followed by 2 feedforward layers, followed by the output layer (see schematic [here](https://www.researchgate.net/profile/Alexander-Khvostikov/publication/322592079/figure/fig3/AS:584350454263818@1516331413967/AlexNet-architecture-Includes-5-convolutional-layers-and-3-fullyconnected-layers.png)). We have loaded in the learned weights and architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgjDJnxOWAHE"
      },
      "source": [
        "# Execute this cell to load in AlexNet model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "model_orig = models.alexnet(pretrained=True)\n",
        "\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            *list(model_orig.features.children())\n",
        "        )\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "        self.classifier = nn.Sequential(\n",
        "            *list(model_orig.classifier.children())\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        activations = {}\n",
        "        conv_features = [2, 5, 7, 9, 12]\n",
        "        for i_feature in range(len(model_orig.features)):\n",
        "            x = self.features[i_feature](x)\n",
        "            if i_feature in conv_features:\n",
        "              activations[f'conv.{conv_features.index(i_feature)}'] = x\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        classifier_features = [2, 5]\n",
        "        for i_classifier in range(len(model_orig.classifier)):\n",
        "            x = self.classifier[i_classifier](x)\n",
        "            if i_feature in classifier_features:\n",
        "              activations[f'feedforward.{classifier_features.index(i_feature)}'] = x\n",
        "\n",
        "        return x, activations\n",
        "\n",
        "model = AlexNet()\n",
        "model.eval()\n",
        "\n",
        "model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uz4yqSMuYXu7"
      },
      "source": [
        "We can push our images through the model and record the unit activations. Let's push 100 images through below.\n",
        "\n",
        "\n",
        "We end up with a dictionary `CNN_acts`. To get the activations of the first layer to the 100 images, you can use `CNN_acts['conv.0']`. This is an array of size (100, 64, 27, 27). We have 64 x 27 x 27 = 46656 units responding to 100 images. For those more familiar with convolutions, this corresponds to 64 different convolutional filters, and 27 x 27 units per filter.\n",
        "\n",
        "To get the activations of the second layer you can use `CNN_acts['conv.1']`, and so on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZIoYPkTb5jF"
      },
      "source": [
        "# Normalize the images for the deep network\n",
        "reshaped_images = images.reshape((7*7*40, 3, 256, 256))/255.\n",
        "reshaped_images = reshaped_images[:, :, 15:-15, 15:-15]\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.44, 0.44, 0.44],\n",
        "                                 std=[0.18, 0.18, 0.18])\n",
        "\n",
        "tensor_images = torch.tensor(reshaped_images).float()\n",
        "\n",
        "tensor_images = normalize(tensor_images)\n",
        "\n",
        "\n",
        "# Push images through model\n",
        "outputs, CNN_acts = model(tensor_images[:100])\n",
        "\n",
        "print(CNN_acts['conv.0'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXFGbf_UeJ_Q"
      },
      "source": [
        "We'll load in the responses for all 1960 images below. It requires some ugly code to avoid running out of memory (and will take a few minutes)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AXf0OaJeJLr"
      },
      "source": [
        "for i_image in range(0, 1960, 196):\n",
        "  \n",
        "  outputs, these_CNN_acts = model(tensor_images[i_image:i_image + 196])\n",
        "\n",
        "  if i_image == 0:\n",
        "    CNN_acts = these_CNN_acts\n",
        "    for k in CNN_acts.keys():\n",
        "      CNN_acts[k] = CNN_acts[k].detach().numpy()\n",
        "  else:\n",
        "     for k in CNN_acts.keys():\n",
        "         CNN_acts[k] = np.concatenate((CNN_acts[k], these_CNN_acts[k].detach().numpy()), axis = 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsmSjR-Rf6Ge"
      },
      "source": [
        "print(CNN_acts['conv.0'].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZO7rzqPZsWfL"
      },
      "source": [
        "### Exercise 3: RDMs of CNN activations \n",
        "\n",
        "Let's compute and plot the RDMs of each of our CNN convolutional layers and for Area IT.\n",
        "\n",
        "A) **(Optional, advanced, if there's time)** Compute and visualize the RDM for the 4th layer of the conv net yourself. Then (even more optional), do it for all of the convolutional layers, 0 through 4!\n",
        "\n",
        "If you don't complete A, you can run the solution to answer the following questions:\n",
        "\n",
        "B) Which convolutional layer's representations most resemble those in area IT?\n",
        "\n",
        "C) What would you conclude about visual processing in AlexNet vs primate brains? Why?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPRKL7P-jY66",
        "cellView": "form"
      },
      "source": [
        "# @markdown Click here for solution\n",
        "\n",
        "# Compute RDM for each convolutional layer\n",
        "conv_rdms = {}\n",
        "for i_layer in range(5):\n",
        "\n",
        "  this_layer_acts = np.mean(CNN_acts[f'conv.{i_layer}'].reshape((7, 7, 40, -1)), axis = 2).reshape((7*7, -1))\n",
        "  conv_rdms[i_layer] = compute_RDM(this_layer_acts)\n",
        "\n",
        "# Visualize all RDMS\n",
        "fig, axes = plt.subplots(1, 7, figsize = (15, 10), sharey = True)\n",
        "\n",
        "for i_layer in range(5):\n",
        "\n",
        "    axes[i_layer].imshow(conv_rdms[i_layer], cmap = 'jet')\n",
        "    for i_cat in range(0, 49, 7):\n",
        "      axes[i_layer].plot([i_cat, i_cat], [0, 49], 'k')\n",
        "      axes[i_layer].plot([0, 49], [i_cat, i_cat], 'k')\n",
        "    axes[i_layer].set(xticks = np.arange(4, 49, 7))\n",
        "    axes[i_layer].set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "    axes[i_layer].set(yticks = np.arange(4, 49, 7))\n",
        "    axes[i_layer].set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "    axes[i_layer].set(xlim = [0, 49], ylim = [49, 0])\n",
        "    axes[i_layer].set(title = f'Conv layer {i_layer}')\n",
        "\n",
        "i_layer = 5\n",
        "axes[i_layer].imshow(V4_rdm, cmap = 'jet')\n",
        "for i_cat in range(0, 49, 7):\n",
        "  axes[i_layer].plot([i_cat, i_cat], [0, 49], 'k')\n",
        "  axes[i_layer].plot([0, 49], [i_cat, i_cat], 'k')\n",
        "axes[i_layer].set(xticks = np.arange(4, 49, 7))\n",
        "axes[i_layer].set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "axes[i_layer].set(yticks = np.arange(4, 49, 7))\n",
        "axes[i_layer].set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "axes[i_layer].set(xlim = [0, 49], ylim = [49, 0])\n",
        "axes[i_layer].set(title = f'Area V4');\n",
        "\n",
        "i_layer = 6\n",
        "axes[i_layer].imshow(IT_rdm, cmap = 'jet')\n",
        "for i_cat in range(0, 49, 7):\n",
        "  axes[i_layer].plot([i_cat, i_cat], [0, 49], 'k')\n",
        "  axes[i_layer].plot([0, 49], [i_cat, i_cat], 'k')\n",
        "axes[i_layer].set(xticks = np.arange(4, 49, 7))\n",
        "axes[i_layer].set_xticklabels(categories, rotation = 45, ha=\"right\")\n",
        "axes[i_layer].set(yticks = np.arange(4, 49, 7))\n",
        "axes[i_layer].set_yticklabels(categories, rotation = 45, ha=\"right\")\n",
        "axes[i_layer].set(xlim = [0, 49], ylim = [49, 0])\n",
        "axes[i_layer].set(title = f'Area IT');\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0seSpeXvK1Z"
      },
      "source": [
        "Your answer here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Click here for B/C solutions\n",
        "\n",
        "\"\"\"\n",
        "B) The fourth convolutional layer most resembles area IT - it has the most block\n",
        "like structure, especially in the diagonal blocks. I think these two RDMs look \n",
        "pretty similar.\n",
        "\n",
        "C) There seems to be a similar representation of information in higher level \n",
        "layers of AlexNet and in higher level areas of the visual system, area IT. This\n",
        "indicates a similarity between the visual information processing in artificial \n",
        "and biological neural networks.\n",
        "\"\"\";\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "FtvISXUU0G98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11vtVdWZinJC"
      },
      "source": [
        "### Exercise 4: Examining the convolutional filters of the first layer \n",
        "\n",
        "In each convolutional layer of AlexNet, we have multiple convolutional filters (64). In the example above, imagine that we have 3 convolutional filters - this would make the convolutional layer size (3 x 2 x 2) instead of (2 x 2). Let's look at the learned convolutional filters in the first layer of AlexNet.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the network weights for the first convolutional layer\n",
        "filters = model.features[0].weight.data.numpy()\n",
        "\n",
        "# Normalize for plotting\n",
        "filters = filters - np.min(filters)\n",
        "filters = filters / np.max(filters)\n",
        "                           "
      ],
      "metadata": {
        "id": "k8Ok14qJ1uqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUs-6Nt1iJz_"
      },
      "source": [
        "**Optional, advanced**: Code this visualization yourself! Plot all 64 filters. \n",
        "\n",
        "Hint 1: `imshow` expects colored images to be number of pixels by number of pixels by 3. `np.swapaxes` may be helpful\n",
        "\n",
        "If you don't code this yourself, execute the next cell to see the plot\n",
        "\n",
        "\n",
        "What types of neurons do you see? Do they resemble any of the receptive fields we've discussed (center-surround, simple cells, complex cells)? What do the similarities hint at about biological visual processing vs artifical network visual processing?"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "arHTT4F-2CKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPflkhKfYU0p",
        "cellView": "form"
      },
      "source": [
        "# @markdown Click here to plot filters\n",
        "# Plot the filters\n",
        "fig, axes = plt.subplots(8, 8, figsize = (8, 8), sharex = True, sharey = True)\n",
        "axes = axes.flatten()\n",
        "for i_ax in range(64):\n",
        "  axes[i_ax].imshow(np.swapaxes(filters[i_ax], 0, 2))\n",
        "  axes[i_ax].axis('Off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuSxN7Bb-T4G"
      },
      "source": [
        "### (Optional, very advanced) Exercise 5) Predicting neural responses\n",
        "\n",
        "Try predicting the neural responses from the convolutional activations using linear regression. Which layer best predicts the IT responses? The V4 responses?\n",
        "\n",
        "Hint: Use PLS_regression from sklearn. This is similar to linear regression but will work slightly better with limited data.\n",
        "\n",
        "Hint 2: Separate train vs test images in a smart way. Instead of completely randomly, use 90% of the 40 configurations for each category and object for training data."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jQAdsQah0rC2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}